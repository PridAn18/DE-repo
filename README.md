# DE-repo

# Пет проекты и учебные работы

Добро пожаловать в репозиторий, содержащий мои пет проекты и учебные работы, выполненные в рамках курса Яндекс Практикум. Здесь вы найдете различные проекты, связанные с обработкой данных, аналитикой и разработкой ETL-пайплайнов.

## Структура репозитория

- **01_ClickHouse-dbt**: Проект, связанный с использованием ClickHouse и dbt для построения аналитических решений.
- **02_Streaming-Kafka**: Проект по потоковой обработке данных с использованием Kafka.

## Учебные проекты из Яндекс Практикум

1. [Проект 2-го спринта](https://github.com/PridAn18/de-project-sprint-2-2023.git)  
   Выполнение учебного проекта по добавлению нового источника в DWH.  
   Содержимое репозитория:
   - Скрипт переноса данных из источника в хранилище.
   - Скрипт для DDL новой витрины.
   - Скрипт для инкрементального обновления витрины.

2. [Проект 3-го спринта](https://github.com/PridAn18/de-project-sprint-3.git)  
   Проектная работа по ETL и автоматизации подготовки данных.  
   Новые инкременты с информацией о продажах приходят по API.  
   Загружаем файлы в таблицы staging, трансформируем данные в таблицы фактов и измерений, формируем витрину данных.

3. [Проект 5-го спринта](https://github.com/PridAn18/de-project-sprint-5.git)  
   Проектная работа по DWH для нескольких источников: MongoDB, API, PostgreSQL.

4. [Проект 7-го спринта](https://github.com/PridAn18/de-project-sprint-7.git)  
   Проектная работа по организации Data Lake: HDFS, PySpark, AirFlow.

5. [Проект 8-го спринта](https://github.com/PridAn18/de-project-sprint-8.git)  
   Проектная работа по потоковой обработке данных: Kafka, PostgreSQL с помощью Spark Structured Streaming и Python в режиме реального времени.

6. [Проект 9-го спринта](https://github.com/PridAn18/de-project-sprint-9.git)  
   Проект по облачным технологиям YandexCloud.  
   Реализует 3 ETL сервиса в Kubernetes:
   - stg_service: Читает сырые данные из Kafka и Redis, объединяет их, отправляет данные в Staging DWH и Kafka для обработки следующим сервисом.
   - dds_service: Читает данные из Kafka, заполняет DDS слой в DWH, создает сообщение для Kafka для заполнения CDM слоя.
   - cdm_service: Читает данные из Kafka, заполняет CDM слой DWH.

7. [Итоговый проект](https://github.com/PridAn18/de-project-final.git)  
   Проект реализует пайплайн обработки данных.  
   Выгрузка данных из S3 в DWH с помощью DAG в Airflow: обработка информации в рамках ETL-процесса. Хранилище реализовано на Vertica. Дашборд реализован на Metabase.


